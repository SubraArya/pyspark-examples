{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "app2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGcIUw4UlG6m17ldrLcrGt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SubraArya/pyspark-examples/blob/master/app2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyMXwm3XjiGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install java and spark run time on the target machine\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q  http://mirror.its.dal.ca/apache/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xvf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-tyVxGlYp58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#setup te environment variables.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oDwhg9G6xTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Upload the files from local machine to EC2 instance , What you like to use in your pyspark program\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_nBJIga7bL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## List and see what is in your  current dir in EC2 \n",
        "print (os.listdir('./'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "addpoahnZbeG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "0ed8eee6-e536-4995-a257-016e74d5c824"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "df = spark.read.json(\"./people.json\")\n",
        "df.show()\n",
        "df.printSchema()\n",
        "df.select(\"name\").show()\n",
        "df.select(df['name'], df['age'] + 1).show()\n",
        "df.filter(df['age'] > 21).show()\n",
        "df.groupBy(\"age\").count().show()\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n",
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n",
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "+-------+\n",
            "\n",
            "+-------+---------+\n",
            "|   name|(age + 1)|\n",
            "+-------+---------+\n",
            "|Michael|     null|\n",
            "|   Andy|       31|\n",
            "| Justin|       20|\n",
            "+-------+---------+\n",
            "\n",
            "+---+----+\n",
            "|age|name|\n",
            "+---+----+\n",
            "| 30|Andy|\n",
            "+---+----+\n",
            "\n",
            "+----+-----+\n",
            "| age|count|\n",
            "+----+-----+\n",
            "|  19|    1|\n",
            "|null|    1|\n",
            "|  30|    1|\n",
            "+----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF8gPXNwbAui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6b017b48-3c11-4005-8e74-7bde4b29018f"
      },
      "source": [
        "# Register the DataFrame as a global temporary view\n",
        "df.createGlobalTempView(\"people\")\n",
        "\n",
        "# Global temporary view is tied to a system preserved database `global_temp`\n",
        "spark.sql(\"SELECT * FROM global_temp.people\").show()\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1gPI50DbL1z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f62900ac-7c34-4c8e-80fc-0a1760aa5399"
      },
      "source": [
        "# Global temporary view is cross-session\n",
        "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-ivVfmdbkKi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "916824cc-498f-4835-bd4d-782ea1f54da9"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Load a text file and convert each line to a Row.\n",
        "lines = sc.textFile(\"./people.txt\")\n",
        "parts = lines.map(lambda l: l.split(\",\"))\n",
        "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
        "\n",
        "# Infer the schema, and register the DataFrame as a table.\n",
        "schemaPeople = spark.createDataFrame(people)\n",
        "schemaPeople.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# SQL can be run over DataFrames that have been registered as a table.\n",
        "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
        "\n",
        "# The results of SQL queries are Dataframe objects.\n",
        "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
        "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
        "for name in teenNames:\n",
        "    print(name)\n",
        "# Name: Justin"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: Justin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oATBlrQHeDUR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4bbef91-db28-4188-f4aa-0f4b9586111b"
      },
      "source": [
        "lines = sc.textFile(\"./people.txt\")\n",
        "lineLengths = lines.map(lambda s: len(s))\n",
        "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
        "print(\"totallength =\",totalLength)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "totallength = 29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slEjsIKtfUyP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "00cfaaf2-302d-4594-e00a-08533ccc9a9d"
      },
      "source": [
        "\"\"\"SimpleApp.py\"\"\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "logFile = \"./rows_10000.csv\"  # Should be some file on your system\n",
        "logData = spark.read.text(logFile).cache()\n",
        "\n",
        "numAs = logData.filter(logData.value.contains('a')).count()\n",
        "numBs = logData.filter(logData.value.contains('b')).count()\n",
        "\n",
        "print(\"Total Lines with \" ,logData.count())\n",
        "print(\"count with A's  = \",numAs)\n",
        "print(\"count with B's  = \",numBs)\n",
        "\n",
        "spark.stop()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Lines with  10000\n",
            "count with A's  =  9904\n",
            "count with B's  =  5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}